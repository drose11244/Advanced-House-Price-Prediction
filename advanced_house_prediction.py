# -*- coding: utf-8 -*-
"""Advanced_House_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sUbRhTeJwKKAqn2hm5lxGALSna2mfjsM
"""

import zipfile
with zipfile.ZipFile('./house-prices-advanced-regression-techniques.zip', 'r') as zip_ref:
    zip_ref.extractall('./')

import pandas as pd
import numpy as np

train_data=pd.read_csv('./train.csv')
test_data=pd.read_csv('./test.csv')

train_data.head(10)

test_data.head()

train_data.info()

test_data.info()

## Top 20 Fields of train data having null values
train_data.isnull().sum().sort_values(ascending=False).iloc[:20]

## Top 35 Fields of test data having null values
test_data.isnull().sum().sort_values(ascending=False).iloc[:35]

len(train_data),len(test_data)

train_data.columns

test_data['Alley'].isnull().sum()

def filter(dataset):
    n=len(dataset)
    ll=[]
    for col in dataset.columns:
        if( dataset[col].isnull().sum()>=int(n*0.2) ):
            ll.append(col)
    print(ll,"features removed from provided data")
    dataset.drop(ll,axis=1,inplace=True)
    return dataset

train_data = filter(train_data)
test_data = filter(test_data)

train_data.isnull().sum().sort_values(ascending=False).iloc[:20]

## Top 35 Fields of test data having null values
test_data.isnull().sum().sort_values(ascending=False).iloc[:35]

train_data.info()

test_data.info()

from sklearn.neighbors import KNeighborsClassifier 

knn = KNeighborsClassifier(n_neighbors=7) 
  
knn.fit(X_train, y_train) 
  
# Predict on dataset which model has not seen before 
print(knn.predict(X_test))

def filter1(dataset):
    for col in dataset.columns:
        #print(dataset[col].dtypes)
        if dataset[col].dtypes=='object':
            dataset[col].fillna(col+'99',inplace=True)
        elif dataset[col].dtypes=='int64' or dataset[col].dtypes=='float64':
            dataset[col].fillna(dataset[col].mean(),inplace=True)            

    return dataset

train_data=filter1(train_data)

test_data=filter1(test_data)

## Top 5 Fields of test data having null values
test_data.isnull().sum().sort_values(ascending=False).iloc[:5]

## Top 5 Fields of test data having null values
train_data.isnull().sum().sort_values(ascending=False).iloc[:5]

import seaborn as sns 
import matplotlib.pyplot as plt 
from scipy.stats import norm 

interested_col=[]

for item in train_data.corr().reset_index()[['index','SalePrice']].values:
    if item[1:2]>=0.7 or item[1:2]<=-0.7:
        interested_col.append(item[0:1][0])

interested_col.remove('SalePrice')
print(interested_col)

tt = train_data.columns.append(test_data.columns)
tt=list(dict.fromkeys(tt))

for i in tt:
    if i not in test_data.columns:
        print(i)

train_data_len = len(train_data)
test_data_len = len(test_data)

print(train_data_len,test_data_len)

print(train_data.shape,test_data.shape)

total = pd.concat([train_data.drop(['SalePrice'],axis=1),test_data],axis=0)   # .drop(['SalePrice'],axis=1)
total.shape

column=['MSZoning','Street','LotShape','LandContour','Utilities','LotConfig','LandSlope','Neighborhood',
         'Condition2','BldgType','Condition1','HouseStyle','SaleType',
        'SaleCondition','ExterCond',
         'ExterQual','Foundation','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2',
        'RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType','Heating','HeatingQC',
         'CentralAir',
         'Electrical','KitchenQual','Functional',
         'GarageType','GarageFinish','GarageQual','GarageCond','PavedDrive']


columns=column+interested_col
print(columns)

total = total[columns]
total.shape

def category_onehot_multcols(multcolumns):
    df_final=total
    i=0
    for fields in multcolumns:
        
        print(fields)
        df1=pd.get_dummies(total[fields],drop_first=True)
        
        total.drop([fields],axis=1,inplace=True)
        if i==0:
            df_final=df1.copy()
        else:
            
            df_final=pd.concat([df_final,df1],axis=1)
        i=i+1
       
        
    df_final=pd.concat([total,df_final],axis=1)
        
    return df_final

total=category_onehot_multcols(column)

total.shape

total = total.loc[:,~total.columns.duplicated()]
total.shape

save_cols=total.columns

from sklearn.preprocessing import StandardScaler
x = total.values
x = StandardScaler().fit_transform(x)

x.shape

x=pd.DataFrame(x,columns=save_cols)

from sklearn.decomposition import PCA

pca = PCA(n_components=20)
principalComponents = pca.fit_transform(x)

Df = pd.DataFrame(data = principalComponents, columns = ['pc1', 'pc2','pc3','pc4','pc5','pc6','pc7','pc8','pc9','pc10','pc11', 'pc12','pc13','pc14','pc15','pc16','pc17','pc18','pc19','pc20'])

print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))

# Df.shape  # (2919, 20)

train = x.iloc[:1460]
test = x.iloc[1460:]

print(train.shape,test.shape)

X_train=train
y_train=train_data['SalePrice']

X_test=test

X_train.shape,y_train.shape,X_test.shape

from tensorflow.keras import backend as K
def root_mean_squared_error(y_true, y_pred):
        return K.sqrt(K.mean(K.square(y_pred - y_true)))

from sklearn.ensemble import RandomForestRegressor 
  
 # create regressor object 
regressor = RandomForestRegressor(n_estimators = 500, random_state = 0) 
  
# fit the regressor with x and y data 
regressor.fit(X_train, y_train)

y_pred = regressor.predict(X_test)

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error

regressor = GradientBoostingRegressor(
    max_depth=10,
    n_estimators=500,
    learning_rate=1.0
)

regressor.fit(X_train, y_train)

errors = [root_mean_squared_error(y_train, y_pred) for y_pred in regressor.staged_predict(X_train)]
best_n_estimators = np.argmin(errors)

best_regressor = GradientBoostingRegressor(
    max_depth=2,
    n_estimators=best_n_estimators,
    learning_rate=1.0
)
best_regressor.fit(X_train, y_train)

y_pred = best_regressor.predict(X_test)

import tensorflow as tf

from tensorflow import keras

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LeakyReLU,PReLU,ELU
from tensorflow.keras.layers import Dropout


# Initialising the ANN
classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(50, kernel_initializer = 'he_uniform',  activation='relu',input_dim = 146))
# Adding the second hidden layer
classifier.add(Dense(25, kernel_initializer = 'he_uniform', activation='relu'))
# Adding the third hidden layer
classifier.add(Dense(50, kernel_initializer = 'he_uniform', activation='relu'))
# Adding the output layer
classifier.add(Dense(1, kernel_initializer = 'he_uniform', use_bias=True))

# Compiling the ANN
classifier.compile(loss=root_mean_squared_error, optimizer='Adamax')

# Fitting the ANN to the Training set
model_history=classifier.fit(X_train.values, y_train.values,validation_split=0.20, batch_size = 10, epochs = 760)

import xgboost as xgb

data_dmatrix = xgb.DMatrix(data=X_train,label=y_train)

xg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,max_depth = 5, alpha = 10, n_estimators = 500)

xg_reg.fit(X_train,y_train)

y_pred = xg_reg.predict(X_test)

params = {"objective":"reg:linear",'colsample_bytree': 0.3,'learning_rate': 0.1,
                'max_depth': 5, 'alpha': 10}

cv_results = xgb.cv(dtrain=data_dmatrix, params=params, nfold=3,
                    num_boost_round=50,early_stopping_rounds=10,metrics="rmse", as_pandas=True, seed=123)

cv_results.head()

y_pred=classifier.predict(test)

pred=pd.DataFrame(y_pred)
sub_df=pd.read_csv('sample_submission.csv')
datasets=pd.concat([sub_df['Id'],pred],axis=1)
datasets.columns=['Id','SalePrice']
datasets.to_csv('sample_submission.csv',index=False)

datasets.head()